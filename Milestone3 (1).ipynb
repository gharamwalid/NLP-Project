{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Milestone 3**"
      ],
      "metadata": {
        "id": "0zVG5E2VaGOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gc\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Process dataset in chunks\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    return chunk\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "    del chunk\n",
        "    gc.collect()\n",
        "\n",
        "# Save tokenizer configuration\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/Checkpoints/tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_chunk = process_chunk(sample_chunk)\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_chunk['text_with_artist'])\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])\n",
        "del sample_chunk, sample_sequences\n",
        "gc.collect()\n",
        "\n",
        "# Function to generate padded sequences and targets in chunks\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "        chunk = process_chunk(chunk)\n",
        "        sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "        input_sequences = []\n",
        "        for seq in sequences:\n",
        "            for i in range(1, len(seq)):\n",
        "                n_gram_sequence = seq[:i+1]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "        input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        targets = input_sequences[:, -1]\n",
        "        input_sequences = input_sequences[:, :-1]\n",
        "        for start in range(0, len(input_sequences), batch_size):\n",
        "            end = min(start + batch_size, len(input_sequences))\n",
        "            yield input_sequences[start:end], targets[start:end]\n",
        "        del chunk, sequences, input_sequences, targets\n",
        "        gc.collect()\n",
        "\n",
        "# Create a TensorFlow dataset from the generator\n",
        "def create_tf_dataset(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    generator = lambda: data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "    dataset = tf.data.Dataset.from_generator(generator,\n",
        "                                             output_types=(tf.int32, tf.int32),\n",
        "                                             output_shapes=((None, max_sequence_len-1), (None,)))\n",
        "    return dataset\n",
        "\n",
        "# Load the pre-trained model from Google Drive\n",
        "model_path = '/content/drive/MyDrive/checkpoints/Yousef_trained_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Freeze the base layers if you don't want to retrain them\n",
        "for layer in model.layers[:-1]:  # Keep the last layer trainable\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "checkpoint_dir = '/content/drive/MyDrive/Checkpoints'\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint(os.path.join(checkpoint_dir, 'fine_tuned_model.h5'), save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "batch_size = 128\n",
        "train_dataset = create_tf_dataset(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "steps_per_epoch = 500  # Set this to a reasonable number\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=5,  # Additional epochs for fine-tuning\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the final model after training\n",
        "final_model_path = os.path.join(checkpoint_dir, 'final_trained_model.h5')\n",
        "model.save(final_model_path)\n",
        "print(f\"Model saved to {final_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owgrdmI0jInf",
        "outputId": "407a81a5-b200-4029-ed5e-4827fc2cd456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - ETA: 0s - loss: 11.2569 - accuracy: 0.0127"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r500/500 [==============================] - 2129s 4s/step - loss: 11.2569 - accuracy: 0.0127 - lr: 1.0000e-04\n",
            "Epoch 2/5\n",
            "500/500 [==============================] - ETA: 0s - loss: 11.2018 - accuracy: 0.0224"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r500/500 [==============================] - 2106s 4s/step - loss: 11.2018 - accuracy: 0.0224 - lr: 1.0000e-04\n",
            "Epoch 3/5\n",
            "500/500 [==============================] - ETA: 0s - loss: 11.1456 - accuracy: 0.0353"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r500/500 [==============================] - 2078s 4s/step - loss: 11.1456 - accuracy: 0.0353 - lr: 1.0000e-04\n",
            "Epoch 4/5\n",
            "500/500 [==============================] - ETA: 0s - loss: 11.0829 - accuracy: 0.0336"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r500/500 [==============================] - 2057s 4s/step - loss: 11.0829 - accuracy: 0.0336 - lr: 1.0000e-04\n",
            "Epoch 5/5\n",
            "500/500 [==============================] - ETA: 0s - loss: 11.0293 - accuracy: 0.0297"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r500/500 [==============================] - 2043s 4s/step - loss: 11.0293 - accuracy: 0.0297 - lr: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/Checkpoints/final_trained_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text based on a seed text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        # Convert the current seed_text to a sequence of tokens\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        # Pad the sequence\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        # Predict the next word\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        # Get the predicted word based on probability values\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        # Get the predicted word from the tokenizer's index_word mapping\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        # Check if output_word is valid\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue  # Skip adding an invalid word and do not count it toward words_added\n",
        "\n",
        "    return current_text"
      ],
      "metadata": {
        "id": "TpUTkaET7ncs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}