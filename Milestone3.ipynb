{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4HYBj3PWZJV",
        "outputId": "379299f9-6740-4396-b0bb-bb4a2db97f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "\n",
        "# Function to process a chunk and yield padded sequences\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "    input_sequences = []\n",
        "    for seq in sequences:\n",
        "        for i in range(1, len(seq)):\n",
        "            n_gram_sequence = seq[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences\n",
        "\n",
        "# Function to yield batches of padded sequences and targets\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size):\n",
        "    while True:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "            input_sequences = process_chunk(chunk)\n",
        "            input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "            targets = input_sequences[:, -1]\n",
        "            input_sequences = input_sequences[:, :-1]\n",
        "            for start in range(0, len(input_sequences), batch_size):\n",
        "                end = min(start + batch_size, len(input_sequences))\n",
        "                yield input_sequences[start:end], targets[start:end]\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_sequences = process_chunk(sample_chunk)\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('Yousef_trained_model.h5')\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "train_steps_per_epoch = 10000 // chunksize\n",
        "val_steps_per_epoch = 10000 // chunksize\n",
        "\n",
        "# Fine-tune the model using a generator\n",
        "batch_size = 128\n",
        "train_generator = data_generator(file_path, chunksize, max_sequence_len, batch_size)\n",
        "validation_generator = data_generator(file_path, chunksize, max_sequence_len, batch_size)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps_per_epoch,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(validation_generator, steps=val_steps_per_epoch)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IbI0Z5XAXE7q",
        "outputId": "56901388-19b3-4aca-efc7-cabcd29af339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node sequential/embedding/embedding_lookup defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-2-e4b3f70ae4ac>\", line 21, in <cell line: 21>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py\", line 272, in call\n\nindices[112,390] = 4229 is not in [0, 2834)\n\t [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_train_function_3454]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e4b3f70ae4ac>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential/embedding/embedding_lookup defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-2-e4b3f70ae4ac>\", line 21, in <cell line: 21>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py\", line 272, in call\n\nindices[112,390] = 4229 is not in [0, 2834)\n\t [[{{node sequential/embedding/embedding_lookup}}]] [Op:__inference_train_function_3454]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "niS3muGjXFrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix 1"
      ],
      "metadata": {
        "id": "0zVG5E2VaGOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Process dataset in chunks\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    return chunk\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "\n",
        "# Save tokenizer configuration\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/Checkpoints/tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "# Function to generate padded sequences and targets in chunks\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "        chunk = process_chunk(chunk)\n",
        "        sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "        input_sequences = []\n",
        "        for seq in sequences:\n",
        "            for i in range(1, len(seq)):\n",
        "                n_gram_sequence = seq[:i+1]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "        input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        targets = input_sequences[:, -1]\n",
        "        input_sequences = input_sequences[:, :-1]\n",
        "        for start in range(0, len(input_sequences), batch_size):\n",
        "            end = min(start + batch_size, len(input_sequences))\n",
        "            yield input_sequences[start:end], targets[start:end]\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_sequences = process_chunk(sample_chunk)\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])\n",
        "\n",
        "# Split data into training and test sets incrementally\n",
        "train_sequences, test_sequences = [], []\n",
        "train_targets, test_targets = [], []\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    input_sequences = process_chunk(chunk)\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "    targets = input_sequences[:, -1]\n",
        "    input_sequences = input_sequences[:, :-1]\n",
        "    X_train_chunk, X_test_chunk, y_train_chunk, y_test_chunk = train_test_split(\n",
        "        input_sequences, targets, test_size=0.1, random_state=42\n",
        "    )\n",
        "    train_sequences.extend(X_train_chunk)\n",
        "    train_targets.extend(y_train_chunk)\n",
        "    test_sequences.extend(X_test_chunk)\n",
        "    test_targets.extend(y_test_chunk)\n",
        "\n",
        "X_train = np.array(train_sequences)\n",
        "y_train = np.array(train_targets)\n",
        "X_test = np.array(test_sequences)\n",
        "y_test = np.array(test_targets)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "L8JGX1IgaH8N",
        "outputId": "801486df-cd70-480a-c8d1-4593c94d959f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'artist'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-10e8160a38cb>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0minput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0minput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0minput_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/data_utils.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'artist'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fix2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Process dataset in chunks\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    return chunk\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "\n",
        "# Save tokenizer configuration\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/Checkpoints/tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "# Function to generate padded sequences and targets in chunks\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "        chunk = process_chunk(chunk)\n",
        "        sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "        input_sequences = []\n",
        "        for seq in sequences:\n",
        "            for i in range(1, len(seq)):\n",
        "                n_gram_sequence = seq[:i+1]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "        input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        targets = input_sequences[:, -1]\n",
        "        input_sequences = input_sequences[:, :-1]\n",
        "        for start in range(0, len(input_sequences), batch_size):\n",
        "            end = min(start + batch_size, len(input_sequences))\n",
        "            yield input_sequences[start:end], targets[start:end]\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_chunk = process_chunk(sample_chunk)\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_chunk['text_with_artist'])\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])\n",
        "\n",
        "# Split data into training and test sets incrementally\n",
        "train_sequences, test_sequences = [], []\n",
        "train_targets, test_targets = [], []\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "    input_sequences = []\n",
        "    for seq in sequences:\n",
        "        for i in range(1, len(seq)):\n",
        "            n_gram_sequence = seq[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "    targets = input_sequences[:, -1]\n",
        "    input_sequences = input_sequences[:, :-1]\n",
        "    X_train_chunk, X_test_chunk, y_train_chunk, y_test_chunk = train_test_split(\n",
        "        input_sequences, targets, test_size=0.1, random_state=42\n",
        "    )\n",
        "    train_sequences.extend(X_train_chunk)\n",
        "    train_targets.extend(y_train_chunk)\n",
        "    test_sequences.extend(X_test_chunk)\n",
        "    test_targets.extend(y_test_chunk)\n",
        "\n",
        "X_train = np.array(train_sequences)\n",
        "y_train = np.array(train_targets)\n",
        "X_test = np.array(test_sequences)\n",
        "y_test = np.array(test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwqhSRX-bWlR",
        "outputId": "d616d68a-58d1-45ef-8b89-dd2473adfd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fix2\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/checkpoints/Yousef_trained_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Verify and adjust the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model.layers[0].input_dim = vocab_size  # Adjust input_dim of the embedding layer\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Checkpoints/fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "train_steps_per_epoch = len(X_train) // 128\n",
        "val_steps_per_epoch = len(X_test) // 128\n",
        "\n",
        "# Fine-tune the model using a generator\n",
        "train_generator = data_generator(file_path, chunksize, max_sequence_len, 128, tokenizer)\n",
        "validation_generator = data_generator(file_path, chunksize, max_sequence_len, 128, tokenizer)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps_per_epoch,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2Mpgtcdbg8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fix2\n",
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return current_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Adele Rolling in the deep\"\n",
        "generated_text = generate_text(seed_text, 10, model, tokenizer, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "GcMyZ74WbnA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/checkpoints/Yousef_trained_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Verify and adjust the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model.layers[0].input_dim = vocab_size  # Adjust input_dim of the embedding layer\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Checkpoints/fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "train_steps_per_epoch = len(X_train) // batch_size\n",
        "val_steps_per_epoch = len(X_test) // batch_size\n",
        "\n",
        "# Fine-tune the model using a generator\n",
        "train_generator = data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "validation_generator = data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps_per_epoch,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)"
      ],
      "metadata": {
        "id": "hhrh91FjaP27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fix3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Process dataset in chunks\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    return chunk\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "\n",
        "# Save tokenizer configuration\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/Checkpoints/tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "# Function to generate padded sequences and targets in chunks\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "        chunk = process_chunk(chunk)\n",
        "        sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "        input_sequences = []\n",
        "        for seq in sequences:\n",
        "            for i in range(1, len(seq)):\n",
        "                n_gram_sequence = seq[:i+1]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "        input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        targets = input_sequences[:, -1]\n",
        "        input_sequences = input_sequences[:, :-1]\n",
        "        for start in range(0, len(input_sequences), batch_size):\n",
        "            end = min(start + batch_size, len(input_sequences))\n",
        "            yield input_sequences[start:end], targets[start:end]\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_chunk = process_chunk(sample_chunk)\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_chunk['text_with_artist'])\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])\n",
        "\n",
        "# Split data into training and test sets incrementally\n",
        "train_sequences, test_sequences = [], []\n",
        "train_targets, test_targets = [], []\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "    input_sequences = []\n",
        "    for seq in sequences:\n",
        "        for i in range(1, len(seq)):\n",
        "            n_gram_sequence = seq[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "    targets = input_sequences[:, -1]\n",
        "    input_sequences = input_sequences[:, :-1]\n",
        "    X_train_chunk, X_test_chunk, y_train_chunk, y_test_chunk = train_test_split(\n",
        "        input_sequences, targets, test_size=0.1, random_state=42\n",
        "    )\n",
        "    train_sequences.extend(X_train_chunk)\n",
        "    train_targets.extend(y_train_chunk)\n",
        "    test_sequences.extend(X_test_chunk)\n",
        "    test_targets.extend(y_test_chunk)\n",
        "\n",
        "X_train = np.array(train_sequences)\n",
        "y_train = np.array(train_targets)\n",
        "X_test = np.array(test_sequences)\n",
        "y_test = np.array(test_targets)\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/checkpoints/Yousef_trained_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Verify and adjust the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# If the model's embedding layer does not match the vocab_size, create a new model with the correct vocab_size\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Define the model\n",
        "new_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=max_sequence_len-1),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Transfer weights from the old model to the new model\n",
        "for layer in model.layers:\n",
        "    try:\n",
        "        new_model.get_layer(name=layer.name).set_weights(layer.get_weights())\n",
        "    except:\n",
        "        print(f\"Layer {layer.name} not found or not compatible\")\n",
        "\n",
        "# Save the new model\n",
        "new_model.save(model_path)\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Checkpoints/fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "train_steps_per_epoch = len(X_train) // 128\n",
        "val_steps_per_epoch = len(X_test) // 128\n",
        "\n",
        "# Fine-tune the model using a generator\n",
        "train_generator = data_generator(file_path, chunksize, max_sequence_len, 128, tokenizer)\n",
        "validation_generator = data_generator(file_path, chunksize, max_sequence_len, 128, tokenizer)\n",
        "\n",
        "history = new_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps_per_epoch,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = new_model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return current_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Adele Rolling in the deep\"\n",
        "generated_text = generate_text(seed_text, 10, new_model, tokenizer, max_sequence_len)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8rV9V-2dWGz",
        "outputId": "b9a41d31-6b9a-4916-8272-74835ecbe874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gc\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Process dataset in chunks\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    return chunk\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "    del chunk\n",
        "    gc.collect()\n",
        "\n",
        "# Save tokenizer configuration\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/Checkpoints/tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_chunk = process_chunk(sample_chunk)\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_chunk['text_with_artist'])\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])\n",
        "del sample_chunk, sample_sequences\n",
        "gc.collect()\n",
        "\n",
        "# Function to generate padded sequences and targets in chunks\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "        chunk = process_chunk(chunk)\n",
        "        sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "        input_sequences = []\n",
        "        for seq in sequences:\n",
        "            for i in range(1, len(seq)):\n",
        "                n_gram_sequence = seq[:i+1]\n",
        "                input_sequences.append(n_gram_sequence)\n",
        "        input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        targets = input_sequences[:, -1]\n",
        "        input_sequences = input_sequences[:, :-1]\n",
        "        for start in range(0, len(input_sequences), batch_size):\n",
        "            end = min(start + batch_size, len(input_sequences))\n",
        "            yield input_sequences[start:end], targets[start:end]\n",
        "        del chunk, sequences, input_sequences, targets\n",
        "        gc.collect()\n",
        "\n",
        "# Create a TensorFlow dataset from the generator\n",
        "def create_tf_dataset(file_path, chunksize, max_sequence_len, batch_size, tokenizer):\n",
        "    generator = lambda: data_generator(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "    dataset = tf.data.Dataset.from_generator(generator,\n",
        "                                             output_types=(tf.int32, tf.int32),\n",
        "                                             output_shapes=((None, max_sequence_len-1), (None,)))\n",
        "    return dataset\n",
        "\n",
        "# Load the trained model\n",
        "model_path = '/content/drive/MyDrive/checkpoints/Yousef_trained_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Verify and adjust the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# If the model's embedding layer does not match the vocab_size, create a new model with the correct vocab_size\n",
        "# Define the model\n",
        "new_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=max_sequence_len-1),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    Dropout(0.5),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Transfer weights from the old model to the new model\n",
        "for layer in model.layers:\n",
        "    try:\n",
        "        new_model.get_layer(name=layer.name).set_weights(layer.get_weights())\n",
        "    except:\n",
        "        print(f\"Layer {layer.name} not found or not compatible\")\n",
        "\n",
        "# Save the new model\n",
        "new_model.save(model_path)\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Checkpoints/fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "batch_size = 128\n",
        "train_dataset = create_tf_dataset(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "steps_per_epoch = 500  # Set this to a reasonable number\n",
        "\n",
        "history = new_model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=5,  # Additional epochs for fine-tuning\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "new_model.load_weights('/content/drive/MyDrive/Checkpoints/fine_tuned_model.h5')\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_dataset = create_tf_dataset(file_path, chunksize, max_sequence_len, batch_size, tokenizer)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "test_loss, test_accuracy = new_model.evaluate(test_dataset, steps=steps_per_epoch // 10)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return current_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Adele Rolling in the deep\"\n",
        "generated_text = generate_text(seed_text, 10, new_model, tokenizer, max_sequence_len)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "drUfwaSPevCJ",
        "outputId": "801f6207-52ce-4014-8a75-bf9f4be5920c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer embedding not found or not compatible\n",
            "Layer dense not found or not compatible\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  9/500 [..............................] - ETA: 1:40:09 - loss: 11.1715 - accuracy: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c2af7ed7b4d6>\u001b[0m in \u001b[0;36m<cell line: 134>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m  \u001b[0;31m# Set this to a reasonable number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m history = new_model.fit(\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}