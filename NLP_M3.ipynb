{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "spotify_df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# Handle missing values\n",
        "spotify_df.dropna(inplace=True)\n",
        "spotify_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Tokenize lyrics\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "spotify_df['cleaned_lyrics'] = spotify_df['text'].astype(str).apply(clean_text)\n",
        "spotify_df['cleaned_lyrics_str'] = spotify_df['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Include artist information in the text\n",
        "spotify_df['text_with_artist'] = spotify_df['artist'] + ' ' + spotify_df['cleaned_lyrics_str']\n",
        "\n",
        "# Tokenize the combined text\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(spotify_df['text_with_artist'])\n",
        "sequences = tokenizer.texts_to_sequences(spotify_df['text_with_artist'])\n",
        "\n",
        "# Create input sequences and their corresponding targets\n",
        "input_sequences = []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        n_gram_sequence = seq[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Prepare targets\n",
        "targets = input_sequences[:, -1]\n",
        "input_sequences = input_sequences[:, :-1]\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, targets, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "5tpNMMj3Jm94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d9b6af-5be3-4126-8d69-4a7bcd60a78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Fine-tune the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)"
      ],
      "metadata": {
        "id": "fwcTEddHJtOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return current_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Adele Rolling in the deep\"\n",
        "generated_text = generate_text(seed_text, 10, model, tokenizer, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "adJM1u6RJu_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fix - 1**"
      ],
      "metadata": {
        "id": "mabsTkAK1Lgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token='<UNK>')\n",
        "\n",
        "# Process dataset in chunks\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    return chunk\n",
        "\n",
        "# Fit tokenizer on the entire dataset\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])\n",
        "\n",
        "# Create input sequences and targets in chunks\n",
        "input_sequences = []\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk = process_chunk(chunk)\n",
        "    sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "    for seq in sequences:\n",
        "        for i in range(1, len(seq)):\n",
        "            n_gram_sequence = seq[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIjSK2Hi0wr6",
        "outputId": "5416c44c-b96d-458b-8cc4-a03836506d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Prepare targets\n",
        "targets = input_sequences[:, -1]\n",
        "input_sequences = input_sequences[:, :-1]\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, targets, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "5FOzdNnh03BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('Yousef_trained_model.h5')\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Fine-tune the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "_SYRMGre08le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)"
      ],
      "metadata": {
        "id": "Dk51oJvi1Dpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return current_text\n"
      ],
      "metadata": {
        "id": "nWzHzAmc1GD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "seed_text = \"Adele Rolling in the deep\"\n",
        "generated_text = generate_text(seed_text, 10, model, tokenizer, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "ZKyGMrxE1Hvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fix - 2**"
      ],
      "metadata": {
        "id": "LKeHnnmu4Kwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load dataset in chunks\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'  # Replace with the actual file path\n",
        "chunksize = 10000  # Adjust chunk size as needed\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "# Fit tokenizer on the entire dataset in chunks\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    tokenizer.fit_on_texts(chunk['text_with_artist'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIgPe2x34MyN",
        "outputId": "f38dd4f8-c9ab-481d-c368-4208240f251f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process a chunk and yield padded sequences\n",
        "def process_chunk(chunk):\n",
        "    chunk.dropna(inplace=True)\n",
        "    chunk.drop_duplicates(inplace=True)\n",
        "    chunk['cleaned_lyrics'] = chunk['text'].astype(str).apply(clean_text)\n",
        "    chunk['cleaned_lyrics_str'] = chunk['cleaned_lyrics'].apply(lambda x: ' '.join(x))\n",
        "    chunk['text_with_artist'] = chunk['artist'] + ' ' + chunk['cleaned_lyrics_str']\n",
        "    sequences = tokenizer.texts_to_sequences(chunk['text_with_artist'])\n",
        "    input_sequences = []\n",
        "    for seq in sequences:\n",
        "        for i in range(1, len(seq)):\n",
        "            n_gram_sequence = seq[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences"
      ],
      "metadata": {
        "id": "ozOgrQBY4S8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to yield batches of padded sequences and targets\n",
        "def data_generator(file_path, chunksize, max_sequence_len, batch_size):\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "        input_sequences = process_chunk(chunk)\n",
        "        input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        targets = input_sequences[:, -1]\n",
        "        input_sequences = input_sequences[:, :-1]\n",
        "        for start in range(0, len(input_sequences), batch_size):\n",
        "            end = min(start + batch_size, len(input_sequences))\n",
        "            yield input_sequences[start:end], targets[start:end]\n",
        "\n",
        "# Determine max sequence length by processing a small sample of the dataset\n",
        "sample_chunk = pd.read_csv(file_path, nrows=chunksize, encoding='utf-8')\n",
        "sample_sequences = process_chunk(sample_chunk)\n",
        "max_sequence_len = max([len(x) for x in sample_sequences])"
      ],
      "metadata": {
        "id": "FM_nepuN4V0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets incrementally\n",
        "train_sequences, test_sequences = [], []\n",
        "train_targets, test_targets = [], []\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunksize, encoding='utf-8'):\n",
        "    input_sequences = process_chunk(chunk)\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "    targets = input_sequences[:, -1]\n",
        "    input_sequences = input_sequences[:, :-1]\n",
        "    X_train_chunk, X_test_chunk, y_train_chunk, y_test_chunk = train_test_split(\n",
        "        input_sequences, targets, test_size=0.1, random_state=42\n",
        "    )\n",
        "    train_sequences.extend(X_train_chunk)\n",
        "    train_targets.extend(y_train_chunk)\n",
        "    test_sequences.extend(X_test_chunk)\n",
        "    test_targets.extend(y_test_chunk)\n",
        "\n",
        "X_train = np.array(train_sequences)\n",
        "y_train = np.array(train_targets)\n",
        "X_test = np.array(test_sequences)\n",
        "y_test = np.array(test_targets)"
      ],
      "metadata": {
        "id": "xFv4UKhf4cQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('Yousef_trained_model.h5')\n",
        "\n",
        "# Setup callbacks for early stopping and best model saving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model_checkpoint = ModelCheckpoint('fine_tuned_model.h5', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Fine-tune the model using a generator\n",
        "batch_size = 128\n",
        "train_generator = data_generator(file_path, chunksize, max_sequence_len, batch_size)\n",
        "\n",
        "# Estimate steps per epoch\n",
        "train_steps_per_epoch = len(X_train) // batch_size\n",
        "val_steps_per_epoch = len(X_test) // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,  # Additional epochs for fine-tuning\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss after Fine-Tuning:\", test_loss)\n",
        "print(\"Test Accuracy after Fine-Tuning:\", test_accuracy)"
      ],
      "metadata": {
        "id": "94a4IkmS4g0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    words_added = 0\n",
        "    current_text = seed_text\n",
        "    while words_added < next_words:\n",
        "        token_list = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list_padded, verbose=0).squeeze()\n",
        "        predicted_index = np.argmax(predictions)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        if output_word and output_word.strip():\n",
        "            current_text += ' ' + output_word.strip()\n",
        "            words_added += 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return current_text"
      ],
      "metadata": {
        "id": "8gBgL5kw4kQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "seed_text = \"Adele Rolling in the deep\"\n",
        "generated_text = generate_text(seed_text, 10, model, tokenizer, max_sequence_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "0AY9JPog4nDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}